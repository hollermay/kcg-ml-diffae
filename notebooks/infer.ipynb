{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "sys.path.append(os.path.abspath('./diffae/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import functional as VF\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/drive/folders/1-5zfxT6Gl-GjxM7z9ZO2AHlB70tfmF6V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import ffhq256_autoenc, LitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "conf = ffhq256_autoenc()\n",
    "model = LitModel(conf)\n",
    "\n",
    "# ckpt_path = f'./diffae/checkpoints/{conf.name}/last.ckpt'\n",
    "# state = torch.load(ckpt_path, map_location='cpu')['state_dict']\n",
    "\n",
    "ckpt_path = './diffae_metroid_1.pth'\n",
    "# ckpt_path = './diffae_cg_10.pth'\n",
    "state = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for official ffhq256 dataset\n",
    "\n",
    "img = Image.open('./diffae/imgs_interpolate/1_a.png')\n",
    "\n",
    "# for metroid dataset\n",
    "\n",
    "# img = Image.open('./images/Metroid_Fusion.jpg')\n",
    "# img = img.crop((128, 128, 128 + 512, 128 + 512))\n",
    "\n",
    "# img = Image.open('./images/Metroid_Fusion_2.jpg')\n",
    "# img = img.crop((0, 512, 512, 512 + 512))\n",
    "\n",
    "# for cg dataset\n",
    "\n",
    "# img = Image.open('./images/panda4.jpg')\n",
    "# img = img.crop((256, 50, 512 + 50, 256 + 100))\n",
    "\n",
    "# img = Image.open('./images/reincarnation.jpg')\n",
    "# img = img.crop((256 - 100, 0, 512 + 100, 256 + 200))\n",
    "\n",
    "img = img.resize((256, 256)).convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VF.to_tensor(img)\n",
    "x = x * 2. - 1.\n",
    "x = x.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    cond = model.encode(x)\n",
    "    \n",
    "    xT = model.encode_stochastic(x, cond, T=250)\n",
    "    \n",
    "    pred = model.render(xT, cond, T=20)\n",
    "\n",
    "cond.shape, xT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(torch.cat([\n",
    "    torch.clamp(x[0] * .5 + .5, 0., 1.),\n",
    "    torch.clamp(xT[0] * .125 + .5, 0., 1.),\n",
    "    torch.clamp(pred[0], 0., 1.)\n",
    "], dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for official ffhq256 dataset\n",
    "\n",
    "img = Image.open('./diffae/imgs_interpolate/1_a.png')\n",
    "img_a = img\n",
    "\n",
    "img = Image.open('./diffae/imgs_interpolate/1_b.png')\n",
    "img_b = img\n",
    "\n",
    "# for metroid dataset\n",
    "\n",
    "# img = Image.open('./images/Metroid_Fusion.jpg')\n",
    "# img = img.crop((128, 128, 128 + 512, 128 + 512))\n",
    "# img_a = img\n",
    "\n",
    "# img = Image.open('./images/Metroid_Fusion_2.jpg')\n",
    "# img = img.crop((0, 512, 512, 512 + 512))\n",
    "# img_b = img\n",
    "\n",
    "# for cg dataset\n",
    "\n",
    "# img = Image.open('./images/panda4.jpg')\n",
    "# img = img.crop((256, 50, 512 + 50, 256 + 100))\n",
    "# img_a = img\n",
    "\n",
    "# img = Image.open('./images/reincarnation.jpg')\n",
    "# img = img.crop((256 - 100, 0, 512 + 100, 256 + 200))\n",
    "# img_b = img\n",
    "\n",
    "\n",
    "img_a = img_a.resize((256, 256)).convert('RGB')\n",
    "img_b = img_b.resize((256, 256)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VF.to_tensor(img_a)\n",
    "x = x * 2. - 1.\n",
    "xA = x.unsqueeze(0).to(device)\n",
    "\n",
    "x = VF.to_tensor(img_b)\n",
    "x = x * 2. - 1.\n",
    "xB = x.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    xR = torch.randn_like(xA)\n",
    "    condR = torch.randn((1, 512), dtype=xR.dtype, device=xR.device)\n",
    "    # condR = torch.zeros((1, 512), dtype=xR.dtype, device=xR.device)\n",
    "    \n",
    "    condA = model.encode(xA)\n",
    "    xTA = model.encode_stochastic(xA, condA, T=250)\n",
    "\n",
    "    xR = torch.mean(xTA, dim=[-2,-1], keepdims=True) + xR * torch.std(xTA, dim=[-2,-1], keepdims=True)\n",
    "    condR = torch.mean(condA, dim=[-2,-1], keepdims=True) + condR * torch.std(condA, dim=[-2,-1], keepdims=True)\n",
    "    \n",
    "    predA = model.render(xTA, condA, T=20)\n",
    "    predAR = model.render(xR, condA, T=20)\n",
    "    predAr = model.render(xA, condR, T=20)\n",
    "    \n",
    "    condB = model.encode(xB)\n",
    "    xTB = model.encode_stochastic(xB, condB, T=250)\n",
    "    predB = model.render(xTB, condB, T=20)\n",
    "    predBR = model.render(xR, condB, T=20)\n",
    "    predBr = model.render(xB, condR, T=20)\n",
    "    \n",
    "    predAtoB = model.render(xTA, condB, T=20)\n",
    "    predBtoA = model.render(xTB, condA, T=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(torch.cat([\n",
    "    torch.cat([\n",
    "        torch.clamp(xA[0] * .5 + .5, 0., 1.),\n",
    "        torch.clamp(xTA[0] * .125 + .5, 0., 1.),\n",
    "        torch.clamp(predA[0], 0., 1.),\n",
    "        torch.clamp(predAR[0], 0., 1.),\n",
    "        torch.clamp(predAr[0], 0., 1.),\n",
    "        torch.clamp(predAtoB[0], 0., 1.),\n",
    "    ], dim=2),\n",
    "    torch.cat([\n",
    "        torch.clamp(xB[0] * .5 + .5, 0., 1.),\n",
    "        torch.clamp(xTB[0] * .125 + .5, 0., 1.),\n",
    "        torch.clamp(predB[0], 0., 1.),\n",
    "        torch.clamp(predBR[0], 0., 1.),\n",
    "        torch.clamp(predBr[0], 0., 1.),\n",
    "        torch.clamp(predBtoA[0], 0., 1.)\n",
    "    ], dim=2)\n",
    "], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.tensor(np.linspace(0, 1, 10, dtype=np.float32)).to(cond.device)\n",
    "intp = condA * (1 - alpha[:, None]) + condB * alpha[:, None]\n",
    "\n",
    "def cos(a, b):\n",
    "    a = a.view(-1)\n",
    "    b = b.view(-1)\n",
    "    a = torch.nn.functional.normalize(a, dim=0)\n",
    "    b = torch.nn.functional.normalize(b, dim=0)\n",
    "    return (a * b).sum()\n",
    "\n",
    "theta = torch.arccos(cos(xTA[0], xTB[0]))\n",
    "x_shape = xTA[0].shape\n",
    "intp_x = (torch.sin((1 - alpha[:, None]) * theta) * xTA[0].flatten(0, 2)[None] + torch.sin(alpha[:, None] * theta) * xTB[0].flatten(0, 2)[None]) / torch.sin(theta)\n",
    "intp_x = intp_x.view(-1, *x_shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model.render(intp_x, intp, T=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(torch.cat(list(torch.clamp(pred, 0., 1.)), dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/drive/folders/1-H8WzKc65dEONN-DQ87TnXc23nTXDTYb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates_latent import ffhq256_autoenc_latent, LitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "conf = ffhq256_autoenc_latent()\n",
    "conf.pretrain.path = f'./diffae/checkpoints/ffhq256_autoenc/last.ckpt'\n",
    "conf.latent_infer_path = f'./diffae/checkpoints/ffhq256_autoenc/latent.pkl'\n",
    "\n",
    "model = LitModel(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(f'./diffae/checkpoints/{conf.name}/last.ckpt', map_location='cpu')\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)\n",
    "with torch.no_grad():\n",
    "    imgs = model.sample(8, device=device, T=20, T_latent=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(2, 4, figsize=(4*5, 2*5))\n",
    "ax = ax.flatten()\n",
    "for i in range(len(imgs)):\n",
    "    ax[i].imshow(imgs[i].cpu().permute([1, 2, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movq",
   "language": "python",
   "name": "movq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
