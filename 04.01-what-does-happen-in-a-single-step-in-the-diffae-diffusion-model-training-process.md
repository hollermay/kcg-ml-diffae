# Diffusion Model Training Process

## Step 1: Semantic Encoding
### Input:
- **\(X\)**: The original (clean) image.

### Operation:
- A **Semantic Encoder** (a CNN) processes \(X\) to extract high-level features.

### Output:
- **\(Z_{\text{sem}}\)**: A semantic vector summarizing the global structure and identity of \(X\).

---

## Step 2: Forward Noising Process
### Inputs:
- **\(X\)**: The original image.  
- **\(\epsilon\)**: Gaussian noise sampled from \( \mathcal{N}(0, I) \).  
- **\(t\)**: A randomly chosen diffusion timestep.

### Operation:
- Noise is added to \(X\) using the formula:

  \[
  X_T = \sqrt{\bar{\alpha}_t}\, X + \sqrt{1 - \bar{\alpha}_t}\, \epsilon
  \]

  where:
  - **\(\beta_t\)** is a predefined noise schedule at timestep \(t\).
  - **\(\alpha_t = 1 - \beta_t\)** represents the retained signal fraction at step \(t\).
  - **\(\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s\)** is the cumulative retention factor up to \(t\).

### Output:
- **Noise map \(X_T\)**: A noised version of \(X\) combining the original signal with added noise.

---

## Step 3: Diffusion U-Net – Noise Prediction
### Inputs:
1. **Noise map** \(X_T\) (from the Forward Noising Process).  
2. **Semantic vector** \(Z_{\text{sem}}\) (from the Semantic Encoder).  
3. **Timestep embedding** (derived from \(t\) to condition the network on the diffusion step).

### Operation:
- The **Diffusion U-Net** processes these inputs to predict the noise that was added to \(X\).

### Output:
- **\(\hat{\epsilon}\)**: The predicted noise (**Noise_pred**).

---

## Step 4: Loss Calculation
### Inputs:
- **Target Noise** \(\epsilon\): The actual Gaussian noise from Step 2.  
- **Predicted Noise** \(\hat{\epsilon}\): The output of the Diffusion U-Net.

### Operation:
- Compute the **Mean Squared Error (MSE) loss** between \(\epsilon\) and \(\hat{\epsilon}\):

  \[
  \text{Loss} = \text{MSE}(\epsilon, \hat{\epsilon})
  \]

### Output:
- A **scalar loss value** that quantifies how accurately the model predicted the noise.

---

## Step 5: Backpropagation
### Input:
- The **scalar loss value** from Step 4.

### Operation:
- **Backpropagate** the loss through the network.
- Update both the **Diffusion U-Net** and the **Semantic Encoder** (since \(Z_{\text{sem}}\) is used as a conditioning input).

### Output:
- **Updated model parameters** for both networks.

---

## Overall Summary

### 1. Semantic Encoding:
- **Input:** \(X\)  
- **Output:** \(Z_{\text{sem}}\)

### 2. Forward Noising Process:
- **Inputs:** \(X\), \(\epsilon\), and \(t\)  
- **Formula:**  

  \[
  X_T = \sqrt{\bar{\alpha}_t}\, X + \sqrt{1 - \bar{\alpha}_t}\, \epsilon
  \]

- **Output:** Noise map \(X_T\)

### 3. Diffusion U-Net – Noise Prediction:
- **Inputs:** Noise map \(X_T\), \(Z_{\text{sem}}\), and timestep embedding  
- **Output:** \(\hat{\epsilon}\)

### 4. Loss Calculation & Backpropagation:
- **Input:** \(\epsilon\) and \(\hat{\epsilon}\)  
- **Operation:** Compute **MSE loss** and **backpropagate** to update model parameters.
